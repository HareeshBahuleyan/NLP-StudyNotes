1. concordance - occurance of the argument word in a nltk.text.Text object
2. similar - words similar to the given word
3. common_context - 2 or more words as inputs. Outputs the contexts when the 2 words are used in thhe same fashion
4. dispersion_plot - plot with word on y-axis and location (position(s)) of the word in the text
5. len(text1) - gives the total number of words in the document
6. len(set(text1)) - gives the length of the vocabulary
7. text1.count("word") - count the number of times a the "word" occurs in text1
8. FreqDist - returns the frequency distribution of the words
9. hapaxes - words that occur only once in the entire document
10. collocations - find the most recent bigrams
11. startswith(); endswith()
12. ConditionalFreqDist - input has to be a list of pairs where each pair has the form (condition,event). Usually the condition represents the category

Word Sense Disambiguation
- understand which sense of a word was intended in a given context. Example: serve can be [help with food or drink] or [old an office] or [put ball into play]

Pronoun Resolution
- whom does the pronoun refer to?
	Example:
	a. The thieves stole the paintings. They were subsequently sold.
	b. The thieves stole the paintings. They were subsequently caught.
	c. The thieves stole the paintings. They were subsequently found.
Computational techniques for tackling this problem include anaphora resolution—identifying what a pronoun or noun phrase refers to—and semantic role labeling—identifying how a noun phrase relates to the verb (as agent,
patient, instrument, and so on).

Question Answering

Machine Translation

A lexicon is a collection of words and/or phrases along with associated information.

Stemming - to retrieve the root of the word
Lemma - the word obtained after stemming and stem completion. Essentially the word has a meaning and exists in the dictionary worlist.

1st step in NLP pipeline is tokenization. NLTK has functionalities for word tokenize, sentence tokenize, etc.
2nd step is POS tagging. Parts of speech are also known as word classes or lexical categories.

Creating a POS Tagger. 
1) Default Tagger
2) Regular Expression Tagger
3) Unigram Tagger - Train with sentences that are already tagged, Evaluate performance on test data set (considering each word individually, regardless of the context. Assign the most frequent/likely tag irrespective of the context. Eg: to fall vs the fall)
4) N-gram Tagger - considering the context, the current word together with the part-of-speech tags of the n-1 preceding tokens. Picks the tag that is most likely in that context (in the training data). Note that for example, if the a particular bi-gram was not found in the train data, then it assigns a 'None' Tag

		 -  As n gets larger, the specificity of the contexts increases, as does the chance that the data we wish to tag contains contexts that were not present in the training data. This is known as the sparse data problem, and is quite pervasive in NLP. As a consequence, there is a trade-off between the accuracy and the coverage of our results.
		 - Note: n-gram taggers should not consider context that crosses a sentence boundary. Accordingly, NLTK taggers are designed to work with lists of sentences, where each sentence is a list of words. At the start of a sentence, token(n-1) and preceding tags are set to 'None' (both training and test).
5) Combine Taggers
	- Try Tagging the token with the bigram tagger.
	- If the bigram tagger is unable to find a tag for the token, try the unigram tagger.
	- If the unigram tagger is also unable to find a tag, use a default tagger.
CODE:
t0 = nltk.DefaultTagger('NN')
t1 = nltk.UnigramTagger(train_sents, backoff=t0)
t2 = nltk.BigramTagger(train_sents, backoff=t1)
t2.evaluate(test_sents)


How do we do do better with out-of-vocabulary words (not present in training)?
- Using default tagger, we are unable to make use of the context
- Instead of default tagger, do the following - limit the vocabulary of the first tagger to the most frequent n words, and to replace every other word with a special word UNK (Unknown)
- During training, a unigram tagger will probably learn that UNK is usually a noun. However, the n-gram taggers will detect contexts in which it has some other tag. For example, if the preceding word is to (tagged TO), then UNK will probably be tagged as a verb.

6) Brill tagging
	- kind of transformation-based learning
	- The general idea is very simple: guess the tag of each word, then go back and fix the mistakes. 
	- need annotated training data to figure out whether the tagger's guess is a mistake or not
	- unlike n-gram tagging, it does not count observations but compiles a list of transformational correction rules.
	- rules are generated from a template of the following form: "replace T1 with T2 in the context C"
	- During its training phase, the tagger guesses values for T1, T2 and C, to create thousands of candidate rules. Each rule is scored according to its net benefit: the number of incorrect tags that it corrects, less the number of correct tags it incorrectly modifies.
